%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Arsclassica Article
% LaTeX Template
% Version 1.1 (10/6/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Lorenzo Pantieri (http://www.lorenzopantieri.net) with extensive modifications by:
% Vel (vel@latextemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[
10pt, % Main document font size
a4paper, % Paper type, use 'letterpaper' for US Letter paper
oneside,
%twocolumn, % One page layout (no page indentation)
%twoside, % Two page layout (page indentation for binding and different headers)
headinclude,footinclude, % Extra spacing for the header and footer
BCOR5mm, % Binding correction
]{scrartcl}

\input{structure.tex} % Include the structure.tex file which specified the document structure and layout

\hyphenation{Fortran hy-phen-ation} % Specify custom hyphenation points in words with dashes where you would like hyphenation to occur, or alternatively, don't put any dashes in a word to stop hyphenation altogether

%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR(S)
%----------------------------------------------------------------------------------------

\title{\normalfont\spacedallcaps{Visual SceneMaker integration in Blender}} % The article title

\author{\spacedlowsmallcaps{Timo Gühring \& Janna Herrmann}} % The article author(s) - author affiliations need to be specified in the AUTHOR AFFILIATIONS block

\date{} % An optional date to appear under the author(s)

%----------------------------------------------------------------------------------------

\begin{document}

%----------------------------------------------------------------------------------------
%	HEADERS
%----------------------------------------------------------------------------------------

\renewcommand{\sectionmark}[1]{\markright{\spacedlowsmallcaps{#1}}} % The header for all pages (oneside) or for even pages (twoside)
%\renewcommand{\subsectionmark}[1]{\markright{\thesubsection~#1}} % Uncomment when using the twoside option - this modifies the header on odd pages
\lehead{\mbox{\llap{\small\thepage\kern1em\color{halfgray} \vline}\color{halfgray}\hspace{0.5em}\rightmark\hfil}} % The header style

\pagestyle{scrheadings} % Enable the headers specified in this block

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS & LISTS OF FIGURES AND TABLES
%----------------------------------------------------------------------------------------

\maketitle % Print the title/author/date block

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

%\section*{Abstract} % This section will not appear in the table of contents due to the star (\section*)


%----------------------------------------------------------------------------------------
%	INTRODUCTION
%----------------------------------------------------------------------------------------

\section{Introduction}

In the context of the seminar "Software design and architectures for interactive avatars: a hands-on approach" we developed an integration of Visual SceneMaker\footnote{\url{http://scenemaker.dfki.de/}} in Blender \footnote{\url{https://www.blender.org/}}.
Visual SceneMaker is a software for creating interactive presentations specialized to non-programming users and Blender is a open source 3D creation suite.\\
The integration is about mainly about avatar animation. It is possible, to create scenes in Visual SceneMaker with actors, speech and actions. The action and speech together with some information and the corresponding actor will be send to blender, where an avatar (with the same name as the actor in SceneMaker) performs the actions whenever a message from SceneMaker is retrieved. An actor can be an avatar itself, or objects in the environment, for example a light that can be on, off or dimmed.
%----------------------------------------------------------------------------------------
%	MANUAL
%----------------------------------------------------------------------------------------

\section{Manual}
In order to make the integration of Visual SceneMaker in Blender work, please follow the steps below.
\begin{enumerate}[noitemsep] % [noitemsep] removes whitespace between the items for a compact look
\item Change to your terminal
\item Execute the following commands inside the terminal:
\begin{lstlisting}
$ git clone git@github.com:JeannedArk/seminarsmcomm.git -b presentation
$ cd ./seminarsmcomm
$ sh start.sh
\end{lstlisting}
\item Close the Blender window with the default setup. Select the Blender window with Anna as character. Press „P“, the game mode starts.
\item Visual SceneMaker will automatically open. Click „Open a project“ > select the „UDPForwarder“ project > click the „select“ button
\item Click the Play button
\item Now you can see the character moving to the corresponding actions defined by the graph.
\end{enumerate}

%------------------------------------------------

\section{Implementation}
Our implementation presupposes some constraints especially in the naming of actions and avatars in order to work correctly.
One constraint is, that the name of avatar, which should perform the action, is the same in Blender and Visual SceneMaker. Another one is, that the actions must be named the same in both programs. %Noch was ?

\subsection{Design Choices}
Together with our implementation we had to make several design choices.\\
We decided to JSON because it is a well known and light weight format. Another very important fact is, that there is no need for manual parsing of the retrieved strings messages on python side.\\
We used a command pattern to execute the activities, because of the high grade of flexibility and dynamic and therefore it builds the base for an easy maintenance and extension in the future. Programmatically spoken the command pattern bewares the concept of separation of concerns.

\subsection{Visual SceneMaker}
To make the communication work, there is a class implemented in Visual SceneMaker called UDPForwarder. It parses the information from the scenes in SceneMaker in action activities and speech activities and sends them, formalized as strings in JSON representation, to all known broadcast addresses on this machine. 

\subsection{Blender}
In Blender we needed an abstract class which represents the activities modeled in SceneMaker. This class is implemented by the two subclasses  ActionActivity and SpeechActivity, similar to SceneMaker. For executing the activities we use a command pattern. Currently, only the executing of ActionActivities is implemented.\\
For the communication we implemented a module called SceneMakerCommunication. Here, the data from the current running Visual SceneMaker instance is recieved and mapped to the ActionActivity or the SpeechActivity class. For communication it creates a socket with the defined configuration. The retrieved
activities will be further propagated to Blender. The retrieving of the data is outsourced to another thread. Incoming data is stored by this thread in a threadsafe queue. The update method in the communication module pops the oldest activity from the queue and executes it, if existing.

%----------------------------------------------------------------------------------------
%	RESULTS AND DISCUSSION
%----------------------------------------------------------------------------------------

\section{Results and Future Work}
The result of our work is a flexible and powerful solution for the communication from Visual SceneMaker to Blender. So far, only action activities of the actors were implemented as actions in Blender. Speech activities are currently sent and recognized as speech, but not interpreted as actions by the avatars in Blender. In order to do that, an integration of a Text-to-Speech software, like for example MaryTTS in blender would be necessary. \\
For evaluation, we performed a stress test with a scene in which events will be sent every 5 ms. The result showed that it performed on a local machine very fast, so that there was no recognizable delay in receiving the events. The communication protocol can be variably extended. Communication can be set to arbitrary network addresses.


\end{document}